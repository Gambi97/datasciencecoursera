---
title: "Week2"
author: "Matteo Gambera"
date: "2 aprile 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Caret package

* Data splitting
* Taining testing fun
* moddel comparison 

Ci sono molti algoritmi

* regression 
* random forests 
* e molti altri

```{r}
library(caret); library(kernlab); data(spam)
## p = 0.75 significa il 75% dei dati per il training
inTrain <- createDataPartition(y=spam$type, p = 0.75 , list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
dim(testing)
1 - dim(testing)[1]/dim(spam)[1]
```
Ora creiamo il modello
```{r}
set.seed(32343)
modelfit <- train(type ~ . , data = training , method = "glm");
modelfit
```
Risultato:

* 3451 sono i dati utilizzati
* 57 sono le colonne utilizzate
* 2 le variabili predette
* il miglior modello è basato su resempling e bootstrapped con 25 ripetizioni

Adesso dobbiamo fittare il modello
Prendo il modello finale dall'oggetto modelfit 
```{r}
modelfit$finalModel
```
Ora dobbiamo fare un predict del nostro set di dati test
```{r}
prediction <- predict(modelfit , newdata = testing)
head(prediction, n= 20)
```
Ora vado a confrontare con i risultati reali
```{r}
confusionMatrix(prediction, testing$type)
```

# Data slicing

per creare data trainging and testing
```{r}

## p = 0.75 significa il 75% dei dati per il training
inTrain <- createDataPartition(y=spam$type, p = 0.75 , list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
```
### K-fold
```{r}
set.seed(1)
## k è il numero di folds, list crea un indicizzazione dei fold
## return train = true ritorno il training altrimenti il test
folds <- createFolds(y = spam$type, k = 10 , 
                     list = TRUE, returnTrain = TRUE )
## guardo le dimensioni dei fold
sapply(folds, length)
```
Guardo il primo fold
```{r}
folds[[1]][1:10]
```
### Resampling
```{r}
set.seed(1)
## times è il numero di resample, list crea un indicizzazione dei resample
## return train = true ritorno il training altrimenti il test
folds <- createResample(y = spam$type, times = 10 , 
                     list = TRUE)
## guardo le dimensioni dei fold
sapply(folds, length)
```
```{r}
folds[[1]][1:10]
```
### Time Slices
for forecasting, in cui si vuole avere valori continui nel tempo
```{r}
set.seed(1)
## Creo il time vector
tme <- 1:1000
## Divido in finestre da 20 e voglio predite i 10 valori dopo
folds <- createTimeSlices(y = tme, initialWindow = 20,
                          horizon = 10)
names(folds)
```
controlliamo come sono strutturati i sample
```{r}
folds$train[1]
folds$train[2]
folds$test[1]
folds$test[2]
```
# Training options
### metric options
* RMSE = root mean square error
* Rsquared = R^2 from regression models
* Accuracy dice quanti ne ha predetti giusti
* kappa a measure of concordance

e un sacco di altre cose qui
[link](https://www.coursera.org/learn/practical-machine-learning/lecture/0vwNS/training-options)

# Plotting predictor 
```{r}
library(ISLR); library(ggplot2); library(caret)
data(Wage)
?Wage
```
```{r}
inTrain <- createDataPartition(y=Wage$wage, p = 0.7 , list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training); dim(testing)
```
Confuso per quest dataset
```{r}
featurePlot(x = training[,c("age", "education", "jobclass")],
            y = training$wage,
            plot = "pairs")
```

```{r}
qplot(age,wage, data = training)
```
```{r}
qplot(age, wage,colour = jobclass, data = training)
```

```{r}
gg <- qplot(age,wage,colour = education, data = training)
gg + geom_smooth(method = "lm" , formula = y ~ x)
```
```{r}
library(Hmisc);
## g sceglie in quante parti divedere wage
cutWage <- cut2(training$wage, g = 3)
table(cutWage)
```
```{r}
p1 <- qplot(cutWage, age, data = training, fill = cutWage,
            geom = c("boxplot"))
p1
```
```{r}

p2 <- qplot(cutWage, age, data = training, fill = cutWage,
            geom = c("boxplot", "jitter"))
#grid.arrange(p1,p2,ncol=2)
```
```{r}
t1 <- table(cutWage, training$jobclass)
t1
```
se voglio la tabella con le proporzioni
```{r}
## 1 per le righe, 2 per le colonne
prop.table(t1,1)
```
```{r}
qplot(wage, colour = education, data = training, geom = "density")
```

# Preprocessing

### center and scaling
```{r}
inTrain <- createDataPartition(y=spam$type, p = 0.75 , list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve , main = "", xlab = "ave. capital run length")
```
Ce ne sono veramente pochi con più di 100, difficili da vedere e analizzare, dobbiamo fare pre processing
```{r}
mean(training$capitalAve)
sd(training$capitalAve)
```
La devstd è molto più grande della media, per non, non far capire un cazzo all'algoritmo ML, vanno prima standardizzati
```{r}
traincapave <- training$capitalAve
traincapaves <- (traincapave - mean(traincapave))/sd(traincapave)
mean(traincapaves)
sd(traincapaves)

```
Stessa cosa va fatta per i test
```{r}
testcapave <- testing$capitalAve
testcapaves <- (testcapave - mean(testcapave))/sd(testcapave)
mean(testcapaves)
sd(testcapaves)

```
C'è gia una funzione che si occupa di standardizzare
```{r}
## Gli passiamo tutto tranne il 58 che è l'outcome di cui
## ci stiamo preoccupando, centro ogni variabile e la scalo
preObj <- preProcess(training[,-58], method = c("center", "scale"))
trainCapAves <- predict(preObj, training[,-58])$capitalAve
mean(trainCapAves)
sd(trainCapAves)
```
Passiamo ora al test, prendo il valore calcolato con il preprocessing e lo applico 
al testset
```{r}
testCapAves <- predict(preObj, testing[,-58])$capitalAve
mean(testCapAves)
sd(testCapAves)
```
Posso direttamnete passare il preprocess alla funzione train
```{r}
set.seed(32343)
modelFit <- train( type ~ . , data = training , 
                   preProcess = c("center", "scale"),
                   method = "glm")
modelFit
```
### box-cox transforms
Prende dei dati continui e tenta di renderli come dei dati normali
```{r}
preObj <- preProcess(training[,-58], method = c("BoxCox"))
trainCapAves <- predict(preObj, training[,-58])$capitalAve
par(mfrow = c(2,2)); hist(trainCapAves); hist(training[,-58]$capitalAve);qqnorm(trainCapAves)
```
non è una bella curva gaussiana, infatti all'inizio ho uno spike, con il normal Q-Q plot lo si può vedere allinizio 
### Imputing data
prediction algorithm spesso sbagliano quando ci sono dati mancanti
```{r}
library(RANN)
set.seed(13343)
## Creo dei valori NA
training$capAve <- training$capitalAve
selectNa <- rbinom(dim(training)[1], size = 1, prob = 0.05)==1
training$capAve[selectNa] <- NA 

## Impute and standardize 
## k nearest neighbors trova le k (es 10) data vectors che più 
## assomigliano ai missing values, fanno una media e sostituiscono il NA
preObj <- preProcess(training[,-58], method = "knnImpute")
capAve <- predict(preObj, training[,-58])$capAve

## standardizzo
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth - mean(capAveTruth))/sd(capAveTruth)
quantile(capAve - capAveTruth)
```

# Covariete creation
faccio una compressione delle informazioni contenute in una riga
ad esempio se ho un testo scritto, vado ad estrapolare solo il numero di volte in cui appare "you"
oppure altre cose
```{r}
## per dare più peso alle differenze gli elevo al quadrato, molto utile per ML
spam$capitalAvesq <- spam$capitalAve^2
```
```{r}
inTrain <- createDataPartition(y = Wage$wage, p = 0.7 , list= FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training); dim(testing)
```
Covariate va applicato SOLO al training
```{r}
table(training$jobclass)
```
Per ML è difficle comprendere delle differenze qualitative (tipo nomi: ind/inf)
Quello che si fa è assegnarli una variabile quantitative
```{r}
dummies <- dummyVars( wage ~ jobclass , data = training)
head(predict(dummies, newdata = training))
```
nearZeroVar identifica le variabile che hanno poco variabilità e quindi che non sono dei buoni predittori
```{r}
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
```
ad esempio region non ha variabilità

### basis spline
```{r}
library(splines)
## vado a creare una nuova variabile, che contiene
## ancora age però scalata per facilitare la parte computazionale
## df=3 mi da age,age^2,age^3
bsBasis <- bs(training$age, df = 3)
head(bsBasis)
```
In questo modo posso avvere fit curvi
```{r}
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage , pch=19, cex = 0.5)
points(training$age, predict(lm1,newdata = training), col = "red")
```
Sulla parte del test
vado a predirre dalla variabile bsbasis un nuovo set di dati
in questo modo non sono legate a quelle del trainingset
```{r}
head(predict(bsBasis, age = teasting$age))
```

# Preprocessing with principal components Analysis
Qaundo ho variabili molto correlate tra di loro non ha senso inserirle tutte nel ML

### correleted predictors
```{r}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type, p = 0.75 , list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
## calcolo la correlazione tra tutte le colonne, 58 è outcome
M <- abs(cor(training[,-58]))
## siccome non sono interessato alla correlazione tra se stesse le tolgo
diag(M) <- 0
## seleziono solo quelle con un certo valore
which(M > 0.8, arr.ind = T)
```
fa capire quali cose appaiono molto assieme, ad esempio il numero 857 e 415 (n°tel)
vado a vedere quindi le colonne in cui appaiono 34,32
```{r}
names(spam)[c(34,32)]
```
```{r}
plot(spam[,34],spam[,32])
```
Non è quindi necessario inserirli tutte e due, dobbiamo combinarle con dei pesi 
comne cominarle???
```{r}
x <- 0.71*training$num415 + 0.71*training$num857
y <- 0.71*training$num415 - 0.71*training$num857
plot(x)
plot(y)
plot(x,y)
```
La maggio parte delle informazioni(quindi più variabile) me le da x quindi sommare, il predittore sarà quindi la somma

### related solution PCA/SVD
singular value decomposition
principal component

```{r}
smallspam <- spam[, c(34,32)]
prComp <- prcomp(smallspam)
prComp
plot(prComp$x[,1], prComp$x[,2])
```
molto simile a quello di prima
```{r}
prComp$rotation
```
la prima colonna somma la seconda sottrae

```{r}
## creo una variabile per colorare
typecolor <- ((spam$type == "spam")*1 +1)
## calcola i componenti principali dell'intero dataset
## con log li rendo un po più gaussiani
prComp <- prcomp(log10(spam[,-58]+1))
## non sono più somme ma sono cose più complicate
plot(prComp$x[,1],prComp$x[,2], col = typecolor )
```
Sulle x cè un po di divisione su spam e non spam
### PCA with caret
```{r}
## pcaComp è il numero di componenti principali
preProc <- preProcess(log10(spam[,-58]+1), method = "pca", pcaComp =2)
spamPC <- predict(preProc, log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2], col = typecolor)
```
### preprocessing with PCA
```{r}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type, p = 0.75 , list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]

preProc <- preProcess(log10(training[,-58]+1), method = "pca", pcaComp =2)

trainPC <- predict(preProc, log10(training[,-58]+1))

#modelFit <- train(training$type ~ . , method="glm" , data = trainPC)## bho
```
```{r}
#testPC <- predict(preProc, log10(testing[,-58]+1))
#confusionMatrix(testing$type, predict(modelFit,testPC))
```

# Predicting with Regression

```{r}
data(faithful); set.seed(333)
inTrain <- createDataPartition(y = faithful$waiting, p = 0.5, list = FALSE)
trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]
head(trainFaith)
```

```{r}
plot(trainFaith$waiting, trainFaith$eruptions, pch=19)
```
Andiamo a fittare
ED = b0 + b1WTi + ei
```{r}
lm1 <- lm( data = trainFaith , eruptions ~ waiting)
summary(lm1)
```
intercept estimate è b0
waiting estimate è b1
```{r}
plot(trainFaith$waiting, trainFaith$eruptions, pch=19)
lines(trainFaith$waiting,lm1$fitted ,lwd =3, col = "red")
```
### predict a new value
ED^ = b0^ + b1^WT
non abbiamo errore perchè non sappiamo quanto sia
```{r}
## 80 per individuare il waiting
coef(lm1)[1] + coef(lm1)[2]*80
```

```{r}
newdata <- data.frame(waiting=80)
newdata
paste0("predizione: ",predict(lm1, newdata))
```

```{r}
par(mfrow = c(1,2))
plot(trainFaith$waiting, trainFaith$eruptions, pch=19)
lines(trainFaith$waiting,predict(lm1) ,lwd =3, col = "red")
plot(testFaith$waiting, testFaith$eruptions, pch=19)
lines(testFaith$waiting,predict(lm1, newdata = testFaith) ,lwd =3, col = "red")
```
La linea di regrassione nella figura test è quella ottenuta attraverso i dati del training, si può vedere che non fitta perfettamente

### get training set/test set errors
calcolo RMSE on training
```{r}
## sottraggo ai valori predetti i valori reali del train
sqrt(sum((lm1$fitted - trainFaith$eruptions)^2))
```
calcolo RMSE on test
```{r}
## sottraggo ai valori predetti attravers il train i valori reali del test
sqrt(sum((predict(lm1, newdata = testFaith) - testFaith$eruptions)^2))
```
Abbastanza normale che sia più grande

### prediction intervals
```{r}
## vado a calcolare una nuova predizione pred1 per il test set utilizzando
## il modello lm1 ricavato dal train e gli dico che vvoglio intervallo di predizione
pred1 <- predict(lm1, newdata = testFaith, interval = "prediction")
## ordino i dati per il test set
ord <- order(testFaith$waiting)
plot(testFaith$waiting , testFaith$eruptions, pch =19)
matlines(testFaith$waiting[ord] , pred1[ord,], type = "l", col= c(3,2,2),
         lty = c(1,2,2) , lwd = 3)

```
### same process with caret
```{r}
modfit <- train(eruptions ~ waiting , data = trainFaith, method = "lm")
summary(modfit$finalModel)
```

# Predicting with regression, multiple covariates

```{r}
library(ISLR)
data(Wage); 
## togliamo logwage che è quella che vogliomo predirre
Wage <- subset(Wage, select = -c(logwage))
summary(Wage)
```
```{r}
inTrain <- createDataPartition(y=Wage$wage, p = 0.7 , list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training); dim(testing)
```
Ora si potrebbe fare un bel feature plot
### fit a linear model
ED = b0 + b1age +b2jobclass + yklevelk
jobclass diventa 1 o 0
education diventa 1 2 3 o 4 
```{r}
## in automatico R converte i fattori in numeri
modFit <- train(wage ~ age + jobclass + education, method = "lm", 
                data = training)

finMod <- modFit$finalModel
print(modFit)
print(finMod)
```
### diagnostic 
```{r}
plot(finMod, 1, pch = 19, cex = 0.5, col ="#00000010")
```
Ci sono alcuni valori marcati che sono outlayer e magari da esplorare e trovare qualche predittore che le spiega meglio

### color by variables not used in the model
```{r}
qplot(finMod$fitted, finMod$residuals, colour = race, data = training)

```
può spiegare gli outlayer

### plot by index
```{r}
plot(finMod$residuals , pch =19)
```
L'index identifica in che riga stiamo osservando, se cè un trend o un gruppo outlayer probabillmente si è mancato di aggiungere qualche predittore nel modello

### Predicted versu truth in test set
risultato della predizione con il test set, decido di colorare per vedere se ho mancato qualche predittore
```{r}
pred <- predict(modFit, testing)
## in colour posso mettere qualsiasi variabile non utilizzata
q <- qplot(wage, pred, colour = race, data = testing)
q + geom_abline(intercept = 0,slope = 1, col = "black", lwd= 1)
```
compara il wage del test set con i valori predetti attraverso il training,
la perfezione sarebbe una linea a 45°
Non ha senso poi andare a rifare il modello inserendo questo predittore, è un post mortem analysis per vedere perchè ha fallito il nostroML

### se si vuole utilizzare all covariates
includo tutti i predittori
```{r}
modFitAll <- train(wage ~ . , data = training, method  = "lm")
pred <- predict(modFitAll, testing)
q <- qplot(wage, pred,colour = race, data = testing)
q + geom_abline(intercept = 0,slope = 1, col = "black", lwd= 1)
```
Anche con tutti i predittori alcuni punti non sono ben spiegati
