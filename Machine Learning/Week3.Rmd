---
title: "Week3"
author: "Matteo Gambera"
date: "5 aprile 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predicting with trees
Pro:

* facile da interpretare
* funziona bene con nonlinear settings

con:

* difficile stimare incertezza

### Basic algorithm

1. start con un unico gruppo 
2. trovo la variabile che separa meglio glo outcomes
3. divide in leaves attraverso un node

### Measure of impurity
1.misclassification error
se per esempio c'è una leaf in cui quasi tutti gli stati votano per barak obama il misclassification error è 1-la probabilità che tu voti per barack obama

errori/totale

* 0 perfect purity
* 0.5 no purity

2.gini index
1 - sum(Pk^2) dove k è la classe

* 0 perfect purity
* 0.5 no purity

3. deviance/information gain
è la probabilità  di essee assegnato a una classe k e leaf m, per lgo2 la proabilità di essee assegnato a una classe k e leaf m

* 0 perfect purity
* 1 no purity

al minuto 6.16,[esempio](https://www.coursera.org/learn/practical-machine-learning/lecture/EALzX/predicting-with-trees)

### Esempio : iris data
```{r}
data(iris); library(ggplot2)
names(iris)
```
```{r}
table(iris$Species)
library(caret); library(kernlab); 
inTrain <- createDataPartition(y = iris$Species, p = 0.8 , list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training)
```
```{r}
qplot(Petal.Width, Sepal.Width, colour = Species, data = iris)
```
```{r}
## rpart package per regression and classification trees
modFit <- train(Species ~ . , method = "rpart" , data = training)
print(modFit$finalModel)
```
n = n°di nodi
2. se la lunghezza dei petali < 2.45 ho il 100% di setosa
3. se maggiore di 2,45 50% versicolor 50% virginica
ecc...
si può fare un plot
```{r}
plot(modFit$finalModel, uniform = TRUE,
     main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
```
una visualizzazone migliore
```{r}
library(rattle)
fancyRpartPlot(modFit$finalModel)
```
### Predict new values
utiliziammo il test set e facciamo delle previsioni
```{r}
library(caret); library(kernlab);
pred <- predict(modFit, newdata = testing)
confusionMatrix(testing$Species, pred)
```

# Bagging
bootstrap aggregation
Quando si fittano modelli complicati, se si mediano questi insieme per ottenere un miglior bilanciamento dei bias e delle varianze
l'idea è:

* Fare resample del dataset e ricalcolo la funzione di previsione
* si ha lo stesso bias che si avrebbe fittando ogni modello singolarmente ma si riduce la variabilità
* utile per non linear

### ozone data

```{r}
install.packages("https://cran.r-project.org/src/contrib/Archive/ElemStatLearn/ElemStatLearn_2012.04-0.tar.gz"    ,repos=NULL, method="libcurl")
```

```{r}
library(ElemStatLearn)
data(ozone, package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
```
Cerco di predirre la temperatura in funzione di ozone

### Bagged Loess
```{r}
## creo una matrice
ll <- matrix(NA, nrow = 10, ncol = 155)
## faccio un resample dei dati 10 volte
for(i in 1:10){
    ss <- sample(1:dim(ozone)[1], replace = T)
    ## creo un nuvo set per ogni loop

}
```

Lungo e sbatti usiamo le fun di caret
leggi bene la doc per usarlo
l'idea è:

* prendere il predittore e metterlo in un data frame
* la variabile da predire metterla in un vettore
* li passo alla funzione bag, B è il numero di resample 
* bagcontrol da le caratterstiche per il fit
* fit richiama train fun
* predict predice i valori basandosi sul fit
* aggregate mette insieme i B modelli previsionali fatti da fit e predict e trova la media
```{r}
predictors <- data.frame(ozone = ozone$ozone)
temperature <- ozone$temperature

treebag <- bag(predictors, temperature , B = 10,
               bagControl = bagControl(fit = ctreeBag$fit,
                                       predict = ctreeBag$pred,
                                       aggregate = ctreeBag$aggregate))
```
plottiamo i risultati
```{r}
plot(ozone$ozone, temperature, col = "lightgrey")
points(ozone$ozone, predict(treebag$fits[[1]]$fit, predictors),pch =19 , col = "red")
points(ozone$ozone, predict(treebag$fits[[2]]$fit, predictors),pch =19 , col = "orange")
points(ozone$ozone, predict(treebag, predictors),pch =19 , col = "blue")
```
I rossi sono il fit di una singol conditional regression tree
la media dei 10 regression tree è quella blu

# Random Forest
è un estensione del bagging per i regression tree
l'idea è:

* Bootstrap samples, quindi faccio resample dei dati
* ad ogni split, bootstrap variables
* faccio diversi alberi e faccio la media

Pro:

* Accuracy

Con:

* speed
* interpretabilità, molti alberi con molti bootstrap tra sample and nodes
* overfitting

esempio 
```{r}
data(iris); library(ggplot2);
inTrain <- createDataPartition( y = iris$Species, p=0.7 , list = FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain,]

```
```{r}
library(caret)
## prox da più info
modFit <- train( Species ~ . , data = training , method = "rf",
                 prox = TRUE)
modFit
```

### getting a single tree
```{r}
library(randomForest)
## k è  quale albero
getTree(modFit$finalModel, k = 2);
```
Ogni riga è uno split 

### Class centers
si usa per vedere quale è il centro della class prediction
```{r}
## vado a vedere il confronto tra length e width
irisP <- classCenter(training[,c(3,4)], training$Species, 
                     modFit$finalModel$prox)
## lo converto in dataframe
irisP <- as.data.frame(irisP)
## aggiungo la colonna specie a cui appartengono nel dataframe
irisP$Species <- rownames(irisP)

p <- qplot(Petal.Width, Petal.Length , col = Species , data = training)
p + geom_point(aes(x = Petal.Width, y = Petal.Length, col = Species),
                   size = 5 , shape = 4 ,  data = irisP)
```

### Predict new values
)
```{r}
pred <- predict(modFit, testing)
## crea variabile true o false per dopo
testing$predRight <- pred == testing$Species
confusionMatrix(pred, testing$Species)
```
### Look at the false prediction
```{r}
qplot(Petal.Width, Petal.Length , colour = predRight , 
      data = testing)
```

# Boosting
è il miglior classificatore che si possa usare
l'idea è:

* prendere la maggior parte possibile di predittori deboli
* pesarli e sommarli
* ottenendo un miglior predittore

boosting:

* inizio con un set di classificatori, tutti i possibili tree, tutti i possibili regression model eccc
*creare un classificatore che li combina tutti
* l'idea è minimizzare l'errore nel training set ad ogni iterata
* calcola i pesi basandsi sull'errore
* aumenta i pesi di quelli che ho sbagliato

Adaboost è il più famoso
[esempio](https://www.coursera.org/learn/practical-machine-learning/lecture/9mGzA/boosting)

Wage esempio 
```{r}
library(ISLR); data(Wage); library(ggplot2); library(caret)
Wage <- subset(Wage , select = -c(logwage))
inTrain <- createDataPartition( y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain,]
```
Fit the model
```{r}
modFit <- train(wage ~ . , method = "gbm", data = training, 
                verbose = FALSE)
print(modFit)
```
plot the result
```{r}
qplot(predict(modFit, testing), wage, data = testing)
```

# Model based prediction
l'idea è:

* assumo che i dati seguano un modello probabilistico
* uso bayes's theorem to identify optimal classifiers

... un sacco di roba da imparare

esempio
```{r}
library(caret); library(kernlab); 
inTrain <- createDataPartition(y = iris$Species, p = 0.7 , list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training)
```
build predictions
```{r}
modlba = train(Species ~ . , method = "lda", data = training)
## naive base
modnb = train(Species ~ . , method = "nb" ,data = training)
plda = predict(modlba, testing)
pnb = predict(modnb, testing)
table(plda, pnb)
```

