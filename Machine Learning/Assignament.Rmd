---
title: "Assignment"
author: "Matteo Gambera"
date: "9 aprile 2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Summary
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants.
The goal of the project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.

#Protocol

##Model
Participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:

* Class A: exactly according to the specification,
* Class B: throwing the elbows to the front,
* Class C: lifting the dumbbell only halfway,
* Class D: lowering the dumbbell only halfway,
* Class E: throwing the hips to the front.

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

##Cross-Validation
Will be performed by subsampling the training data set randomly without replacement into 2 subsamples: subTraining data (70% of the original Training data set) and subTesting data (30%). Our models will be fitted on the subTraining data set, and tested on the subTesting data. Once the most accurate model is choosen, it will be tested on the original Testing data set.

##Reproducibility and Library
The pseudo-random number generator seed was set at 1 for all code.
For this project load this library.
```{r}
##Seed
set.seed(1)

##Libraries
library(caret)
library(randomForest)
library(rpart)
library(rattle)
library(RColorBrewer)
library(ggplot2)
```

#Data
The training data for this project are available here:

* **[training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)**

The test data are available here:

* **[testing](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)**

The data for this project come from this source: **[link](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har)**. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

## Getting Data
```{r}
library(data.table)

## Data training
url_data <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
data <- fread(url_data,
              na.strings = c("NA,",""))

## Data testing
url_validation <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
validation <- fread(url_validation, 
                    na.strings = c("NA", ""))
```
##Cleaning Data
look at the structure of training data and delete columns with all missing values or irrelevant data.
```{r}
## Training and testing
MissingValues <- sapply(data, function (x) any(is.na(x) | x == ""))
Predictor <- !MissingValues & grepl("belt|[^(fore)]arm|dumbbell|forearm",
                                    names(MissingValues))
predCandidates <- names(MissingValues)[Predictor]
varToInclude <- c("classe", predCandidates)
data <- data[,varToInclude, with = FALSE]
paste0("dimension data pre cleaning: ",dim(data))
data[,.N,classe]
myDataNZV <- nearZeroVar(data, saveMetrics=TRUE)
data <- subset(data, select = which(myDataNZV$nzv == FALSE))
paste0("dimensione data post cleaning: ",dim(data))



```


##Partition the data set
Partition the data set in two data sets

* training 70%
* testing 30%

```{r}
inTrain <- createDataPartition(y = data$classe, p = 0.7, 
                               list = FALSE)
training <- data[inTrain, ]
testing <- data[-inTrain, ]
dim(training); dim(testing)
```


# Prediction model
We train two classification-orientated models on the training set, using random forests (rf in R’s caret package), boosted trees (gbm).
We train the models using 2-fold cross-validation. In other words, for each model and each candidate combination of tuning parameters (for instance, the tuning parameter for the random forest model is mtry, the number of randomly selected predictors), one fifth of the training data is held out and serves as a cross-validation set on which the accuracy is calculated after the model has been trained on the rest of the data. The best model is then fitted on the full training set.
```{r message=FALSE}
trainCtrl <- trainControl(method = "cv", number = 2)

set.seed(1)
modRf <- train(classe ~ ., method = "rf", data = training,
               trControl = trainCtrl)
set.seed(1)
modGbm <- train(classe ~ ., method="gbm", data = training,
                trControl = trainCtrl)

                
```
Before we use these models to predict classes on the test set, we compare their resampling distributions (noting that there are 2 resamples for each model as we used 2-fold cross-validation) to get a general idea of the difference in performance between the models, and to estimate the out of sample error.
```{r}
# collect resamples
results <- resamples(list(RF=modRf, GBM=modGbm))

# summary of the resampling distributions
summary(results)
```
```{r}
dotplot(results)
```
We now use the models to predict the class of the activities in the testing data set, and calculate the accuracy (or, equivalently, the out of sample error) of the models on this set.
```{r}
# random forest
predRfTest <- predict(modRf, newdata=testing)
confusionMatrix(as.factor(testing$classe), predRfTest)$table

# boosted trees
predGbmTest <- predict(modGbm, newdata=testing)
confusionMatrix(as.factor(testing$classe), predGbmTest)$table


```
After predicting the classes on the testing set, we obtain:

* 99.4% accuracy on the testing set and a kappa (κ) value of 0.99, for the model fitted using random forests.

* 96.3% accuracy  and κ= 0.953 for boosted trees.

The accuracy of the random forest model is excellent, and the accuracy of the boosted trees model is also very good. 

We then fit a model that combines the two best predictors, namely those produced by the random forest model and the boosted trees model, using a random forest, on the test set.
```{r}
# fit the combination
predRfGbmTest <- data.frame(predRf = predRfTest, predGbm = predGbmTest,
                            classe=testing$classe)
modCombRfGbm <- train(classe ~ ., model = "rf", data=predRfGbmTest, 
                 trControl = trainCtrl)
# predict on testing
predCombPredTest <- predict(modCombRfGbm, predRfGbmTest)
```
We use this final model to predict the classes on the validation set, and display the resulting confusion matrix.
```{r}

# first predict using the random forest and boosted trees models
predRfValidation <- predict(modRf, newdata=testing)
predGbmValidation <- predict(modGbm, newdata=testing)

# feed these predictions into our final model
predCombRfGbmValidation <- predict(modCombRfGbm, 
                              newdata=data.frame(predRf = predRfValidation, 
                                                 predGbm = predGbmValidation))

# confusion matrix
cmCombRfGbmValidation <- confusionMatrix(as.factor(testing$classe), 
                                         predCombRfGbmValidation)
cmCombRfGbmValidation$table
```
```{r}
# accuracy, out of sample error, kappa on validation set
accCombRfGbmValidation <- cmCombRfGbmValidation$overall[['Accuracy']]
oseCombRfGbmValidation <- 1-accCombRfGbmValidation
kappaCombRfGbmValidation  <- cmCombRfGbmValidation$overall[['Kappa']]
```
# Predictions
To make the final predictions, we first load the test cases, removing non-available values as we did for the training data file. We then use the previously fitted random forest and boosted trees models to make our first set of predictions, which we then pass to our model that combines the two predictors.
```{r}

predTestCasesRf <- predict(modRf, newdata=validation)
predTestCasesGbm <- predict(modGbm, newdata=validation)

# prediction
predTestCasesCombModFit <- predict(modCombRfGbm, 
                                   newdata = data.frame(
                                     predRf = predTestCasesRf, 
                                     predGbm = predTestCasesGbm))
predTestCasesCombModFit
```

